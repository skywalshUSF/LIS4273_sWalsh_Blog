[
  {
    "path": "posts/2025-03-30-module-11-assignment/",
    "title": "Module-11-Assignment",
    "description": "Logistic Regression",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-03-30",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 3/30/2025\r\n\r\n# 'Question A'\r\n\r\n# Set up an additive model for the ashina data, as part of ISwR package\r\n\r\nlibrary(ISwR)\r\ndata(\"ashina\")\r\nattach(ashina)\r\n\r\n# This data contain additive effects on subjects, period and treatment.\r\n\r\nashina$subject <- factor(1:16)\r\nattach(ashina)\r\nact <- data.frame(vas=vas.active, subject, treat=1, period=grp)\r\nplac <- data.frame(vas=vas.plac, subject, treat=0, period=grp)\r\n\r\n# Combine data frames for treatment groups using rbind()\r\nashinaModel <- rbind(act,plac)\r\n\r\n# Model the outcome \"vas\" predicted by \"treat\" and \"period\"\r\nadditiveModel <- glm(vas ~ treat+period, data = ashinaModel)\r\nsummary(additiveModel)\r\n\r\n\r\nCall:\r\nglm(formula = vas ~ treat + period, data = ashinaModel)\r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept)   -56.75      26.56  -2.136   0.0412 *\r\ntreat         -42.88      16.75  -2.560   0.0159 *\r\nperiod         31.13      17.29   1.800   0.0822 .\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for gaussian family taken to be 2243.164)\r\n\r\n    Null deviance: 87028  on 31  degrees of freedom\r\nResidual deviance: 65052  on 29  degrees of freedom\r\nAIC: 342.56\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\n# The t-value for the \"period\" predictor variable is 1.800 with a p-value\r\n# of 0.0822, which means that \"period\" does not significantly affect \"vas\".\r\n\r\n# The t-value for the \"treat\" predictor variable is -2.560 with a p-value\r\n# of 0.0159, which means that \"treat\" does significantly affect \"vas\".\r\n\r\n# Compare the results with those obtained from t tests.\r\n\r\n# Conduct t tests to determine if \"period\" group 1 and group 2 affect \"vas\".\r\nt1 <- t.test(vas~period, data = ashinaModel)\r\nt1\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  vas by period\r\nt = -1.8995, df = 29.872, p-value = 0.0672\r\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\r\n95 percent confidence interval:\r\n -64.613318   2.346651\r\nsample estimates:\r\nmean in group 1 mean in group 2 \r\n      -47.05000       -15.91667 \r\n\r\n# With a t value of -1.8995 and a p-value of 0.0672, the t test results confirm\r\n# what the model suggested, that \"period\" does not have a statistically\r\n# significant effect on \"vas\".\r\n\r\n# Conduct t tests to determine if \"treat\" active and placebo affect \"vas\".\r\nt2 <- t.test(vas~treat, data = ashinaModel)\r\nt2\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  vas by treat\r\nt = 2.4699, df = 24.063, p-value = 0.02099\r\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\r\n95 percent confidence interval:\r\n  7.052494 78.697506\r\nsample estimates:\r\nmean in group 0 mean in group 1 \r\n       -13.9375        -56.8125 \r\n\r\n# With a t value of 2.4699 and a p-value of 0.02099, the t test results confirm\r\n# what the model suggested, that \"treat\" does have a statistically\r\n# significant effect on \"vas\".\r\n\r\n# 'Question B'\r\n\r\n# Consider the following\r\na <- gl(2, 2, 8) # Creates factor with 2 levels, each repeated 2 times, length 8\r\nb <- gl(2, 4, 8) # Creates factor with 2 levels, each repeated 4 times, length 8\r\nx <- 1:8\r\ny <- c(1:4, 8:5)\r\nz <- rnorm (8)\r\n\r\n# Instruction: The rnorm() is a built-in R function that generates a vector of \r\n# normally distributed random numbers. The rnorm() method takes a sample size \r\n# as input and generates that many random numbers. \r\n# We are looking for two models: (1) model.matrix (z ~ a:b); (2) lm (z ~ a:b).\r\n\r\n# B1. Your assignment is to generate the model matrices for the following models:\r\n\r\n# z ~ a*b  Model with interaction (a*b),\r\ninteractionModel <- model.matrix(~a*b)\r\n# Fit model matrix to linear model\r\nlm.interactionModel <- lm(z ~ a*b)\r\n# z ~ a:b  Model with only interaction term (a:b)).\r\ninteractionTermModel <- model.matrix(~a:b)\r\n# Fit model matrix to linear model\r\nlm.interactionTermModel <- lm(z ~ a:b)\r\n\r\n# B2. Please also discuss the implications of using these two models; \r\n# please be reminded about the model fits and notice which models \r\n# contain singularities.\r\n\r\n# Call the interaction (z ~ a*b) model matrix \r\ninteractionModel\r\n\r\n  (Intercept) a2 b2 a2:b2\r\n1           1  0  0     0\r\n2           1  0  0     0\r\n3           1  1  0     0\r\n4           1  1  0     0\r\n5           1  0  1     0\r\n6           1  0  1     0\r\n7           1  1  1     1\r\n8           1  1  1     1\r\nattr(,\"assign\")\r\n[1] 0 1 2 3\r\nattr(,\"contrasts\")\r\nattr(,\"contrasts\")$a\r\n[1] \"contr.treatment\"\r\n\r\nattr(,\"contrasts\")$b\r\n[1] \"contr.treatment\"\r\n\r\n# This is a design matrix that demonstrates the effect of a and b and a:b on z\r\n\r\n# Call the linear model\r\nsummary(lm.interactionModel)\r\n\r\n\r\nCall:\r\nlm(formula = z ~ a * b)\r\n\r\nResiduals:\r\n      1       2       3       4       5       6       7       8 \r\n 0.5748 -0.5748  1.1115 -1.1115 -2.1016  2.1016  1.0060 -1.0060 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)  -1.1899     1.3224  -0.900    0.419\r\na2            0.5004     1.8701   0.268    0.802\r\nb2            0.7899     1.8701   0.422    0.694\r\na2:b2        -0.3032     2.6447  -0.115    0.914\r\n\r\nResidual standard error: 1.87 on 4 degrees of freedom\r\nMultiple R-squared:  0.07315,   Adjusted R-squared:  -0.622 \r\nF-statistic: 0.1052 on 3 and 4 DF,  p-value: 0.9527\r\n\r\n# Call the interaction term (z ~ a:b) model matrix \r\ninteractionTermModel\r\n\r\n  (Intercept) a1:b1 a2:b1 a1:b2 a2:b2\r\n1           1     1     0     0     0\r\n2           1     1     0     0     0\r\n3           1     0     1     0     0\r\n4           1     0     1     0     0\r\n5           1     0     0     1     0\r\n6           1     0     0     1     0\r\n7           1     0     0     0     1\r\n8           1     0     0     0     1\r\nattr(,\"assign\")\r\n[1] 0 1 1 1 1\r\nattr(,\"contrasts\")\r\nattr(,\"contrasts\")$a\r\n[1] \"contr.treatment\"\r\n\r\nattr(,\"contrasts\")$b\r\n[1] \"contr.treatment\"\r\n\r\n# This is a design matrix that demonstrates the effect of a:b on z\r\n\r\n# Call the linear model\r\nsummary(lm.interactionTermModel)\r\n\r\n\r\nCall:\r\nlm(formula = z ~ a:b)\r\n\r\nResiduals:\r\n      1       2       3       4       5       6       7       8 \r\n 0.5748 -0.5748  1.1115 -1.1115 -2.1016  2.1016  1.0060 -1.0060 \r\n\r\nCoefficients: (1 not defined because of singularities)\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)  -0.2028     1.3224  -0.153    0.886\r\na1:b1        -0.9871     1.8701  -0.528    0.626\r\na2:b1        -0.4867     1.8701  -0.260    0.808\r\na1:b2        -0.1972     1.8701  -0.105    0.921\r\na2:b2             NA         NA      NA       NA\r\n\r\nResidual standard error: 1.87 on 4 degrees of freedom\r\nMultiple R-squared:  0.07315,   Adjusted R-squared:  -0.622 \r\nF-statistic: 0.1052 on 3 and 4 DF,  p-value: 0.9527\r\n\r\n# Both models have the same Multiple R-squared:  0.2457\r\n# The interaction term only model has a singularity\r\n# The coefficient shows NA for a2:b2\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/multivariate.png?raw=true",
    "last_modified": "2025-03-30T13:51:36-04:00",
    "input_file": "module-11-assignment.knit.md"
  },
  {
    "path": "posts/2025-03-26-module-10-assignment/",
    "title": "Module-10-Assignment",
    "description": "Multivariate Regression and Anova",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-03-26",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 3/26/2025\r\n\r\n# 'Question A'\r\n\r\n# Conduct ANOVA (analysis of variance) and Regression coefficients to the \r\n# data from data (\" cystfibr \") database. You can choose any \r\n# variable you like to interpret.\r\n\r\n\r\n# A1. In the report, please state the result of coefficients and significance \r\n# to any variables you like both under ANOVA and multivariate analysis. \r\n# Please provide a specific interpretation of R results.\r\n\r\nlibrary(ISwR)\r\ndata(\"cystfibr\")\r\nattach(cystfibr)\r\n\r\n# Create a linear regression model object\r\n# pemax is predicted by age, weight, bmp, and fev1 combined\r\npemaxModel <- lm(pemax ~ age + weight + bmp + fev1, data = cystfibr)\r\nsummary(pemaxModel)\r\n\r\n\r\nCall:\r\nlm(formula = pemax ~ age + weight + bmp + fev1, data = cystfibr)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-42.521 -10.885   3.003  15.488  41.767 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)   \r\n(Intercept) 179.2957    61.8855   2.897  0.00891 **\r\nage          -3.4181     3.3086  -1.033  0.31389   \r\nweight        2.6882     1.1727   2.292  0.03287 * \r\nbmp          -2.0657     0.8198  -2.520  0.02036 * \r\nfev1          1.0882     0.5139   2.117  0.04695 * \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 23.4 on 20 degrees of freedom\r\nMultiple R-squared:  0.5918,    Adjusted R-squared:  0.5101 \r\nF-statistic: 7.248 on 4 and 20 DF,  p-value: 0.0008891\r\n\r\n# The (Intercept), 179.2957, is the predicted value of the response\r\n# variable \"pemax,\" which is the maximum expiratory pressure when\r\n# the predictor variables (age, weight, bmp, fev1) are zero.\r\n\r\n# The coefficient for the \"age\" variable is -3.4181, which suggests\r\n# that for each unit \"age\" increases, \"pemax\" decreases by 3.4181 units.\r\n\r\n# The coefficient for the \"weight\" variable is 2.6882, which suggests\r\n# that for each unit \"weight\" increases, \"pemax\" increases by 2.6882 units.\r\n\r\n# The coefficient for the \"bmp\" variable is -2.0657, which suggests\r\n# that for each unit \"bmp\" increases, \"pemax\" decreases by 2.0657 units.\r\n\r\n# The coefficient for the \"fev1\" variable is 1.0882, which suggests\r\n# that for each unit \"fev1\" increases, \"pemax\" increases by 1.0882 units.\r\n\r\n# The summary() suggests that \"weight,\" \"bmp,\" and \"fev1,\" are statistically\r\n# significant because the p value is less than 0.05% alpha\r\n# A high F-statistic of 7.248 and an extremely small p-value 0.0008891\r\n# suggest that at least one of the predictor variables has a significant effect\r\n# on the outcome variable.\r\n\r\n# Conduct ANOVA analysis to analyze the results of the regression model\r\n# with multiple variables to assess whether there are differences in group\r\n# means and variance that identify significant relationships among variables. \r\nanova(lm(pemax ~ age + weight + bmp + fev1, data=cystfibr))\r\n\r\nAnalysis of Variance Table\r\n\r\nResponse: pemax\r\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \r\nage        1 10098.5 10098.5 18.4385 0.0003538 ***\r\nweight     1   945.2   945.2  1.7258 0.2038195    \r\nbmp        1  2379.7  2379.7  4.3450 0.0501483 .  \r\nfev1       1  2455.6  2455.6  4.4836 0.0469468 *  \r\nResiduals 20 10953.7   547.7                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n# After conducting the ANOVA analysis, the Analysis of Variance Table suggests:\r\n\r\n# for, Null hypothesis Ho: there is no difference between group means\r\n#      Alt  hypothesis Ha: there is a difference between group means\r\n\r\n# The \"age\" variable has a high F-value of 18.4385 and a small p-value 0.0003538\r\n# Therefore, we can reject the null hypothesis and conclude that the \"age\"\r\n# variable has a statistically significant effect on the response variable\r\n\r\n# The \"weight\" variable has a low F-value of 1.7258 and large p-value 0.2038195\r\n# Therefore, we would fail to reject the null hypothesis and conclude that the\r\n# \"weight\" variable does not have a statistically significant effect on the\r\n# response variable. This indicates that a model without the \"weight\" variable\r\n# would fit as well as the model with the \"weight\" variable.\r\n\r\n# The \"bmp\" and \"fev1\" variables are very close to the 0.05% alpha level of\r\n# significance and their p-values suggest that they may have a statistically\r\n# significant effect on the response variable. With a p-value of 0.0501483\r\n# for \"bmp\" we fail to reject the null and a p-value of 0.0469468 for \"fev1\"\r\n# we can reject the null.\r\n\r\n\r\n# 'Question B'\r\n\r\n\r\n# The secher data(\"secher\") are best analyzed after log-transforming \r\n# birth weight as well as the abdominal and biparietal diameters. \r\n# Fit a prediction weight as well as abdominal and biparietal diameters. \r\n# For a prediction equation for birth weight.\r\n\r\n# B1. How much is gained by using both diameters in a prediction equation? \r\n# The sum of the two regression coefficients is almost identical and equal to 3.\r\n\r\n# Load the data set\r\ndata(\"secher\")\r\nattach(secher)\r\n\r\n# Model with only abdominal diameter\r\nmodel_ad <- lm(log(bwt) ~ I(log(ad)), data=secher)\r\nsummary(model_ad)\r\n\r\n\r\nCall:\r\nlm(formula = log(bwt) ~ I(log(ad)), data = secher)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.58560 -0.06609  0.00184  0.07479  0.48435 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -2.4446     0.5103  -4.791 5.49e-06 ***\r\nI(log(ad))    2.2365     0.1105  20.238  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.1275 on 105 degrees of freedom\r\nMultiple R-squared:  0.7959,    Adjusted R-squared:  0.794 \r\nF-statistic: 409.6 on 1 and 105 DF,  p-value: < 2.2e-16\r\n\r\n# Model with only biparietal diameter\r\nmodel_bpd <- lm(log(bwt) ~ I(log(bpd)), data=secher)\r\nsummary(model_bpd)\r\n\r\n\r\nCall:\r\nlm(formula = log(bwt) ~ I(log(bpd)), data = secher)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.36478 -0.09725  0.01251  0.07703  0.51154 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -7.0862     0.9062  -7.819 4.35e-12 ***\r\nI(log(bpd))   3.3320     0.2017  16.516  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.1488 on 105 degrees of freedom\r\nMultiple R-squared:  0.7221,    Adjusted R-squared:  0.7194 \r\nF-statistic: 272.8 on 1 and 105 DF,  p-value: < 2.2e-16\r\n\r\n# Combine both models\r\nmodel_combined <- lm(log(bwt) ~ I(log(ad)) + I(log(bpd)), data=secher)\r\nsummary(model_combined)\r\n\r\n\r\nCall:\r\nlm(formula = log(bwt) ~ I(log(ad)) + I(log(bpd)), data = secher)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.35074 -0.06741 -0.00792  0.05750  0.36360 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -5.8615     0.6617  -8.859 2.36e-14 ***\r\nI(log(ad))    1.4667     0.1467   9.998  < 2e-16 ***\r\nI(log(bpd))   1.5519     0.2294   6.764 8.09e-10 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.1068 on 104 degrees of freedom\r\nMultiple R-squared:  0.8583,    Adjusted R-squared:  0.8556 \r\nF-statistic: 314.9 on 2 and 104 DF,  p-value: < 2.2e-16\r\n\r\n# Using both abdominal and biparietal diameters in the prediction equation \r\n# increases the accuracy compared to using either diameter alone. \r\n# The r-squared value increases as more variables are added. The sum of \r\n# the regression coefficients being approximately 3 indicates that a 1% \r\n# increase in both diameters is associated with a 3% increase in birth weight.\r\n\r\n# B2. Can this be given a nice interpretation to our analysis? \r\n# Please provide step by step on your analysis and code you use \r\n# to find out the result. \r\n\r\n# R Squared is the coefficient of determination that ranges from 0-1\r\n# and determines the proportion of the variation in the dependent variable \r\n# that is explained by the independent variables in the regression model.\r\nr_squared_combined <- summary(model_combined)$r.squared\r\nr_squared_ad <- summary(model_ad)$r.squared\r\nr_squared_bpd <- summary(model_bpd)$r.squared\r\n\r\n# Increasing R squared means that the model will explain a greater percentage \r\n# of the variability in the dependent variable. Higher R Squared values \r\n# indicate a better fit of the model to the data.\r\n\r\n# When comparing the R Squared difference between the 'ad' only model\r\n# and the 'ad' and 'bpd' combined model:\r\nadDiff<-r_squared_combined - r_squared_ad\r\ncat(\"There is a\",adDiff,\"increase in r squared\r\nFrom\",r_squared_ad,\"to\",r_squared_combined)\r\n\r\nThere is a 0.06234051 increase in r squared\r\nFrom 0.7959412 to 0.8582817\r\n\r\n# When comparing the R Squared difference between the 'bpd' only model\r\n# and the 'ad' and 'bpd' combined model:\r\nbpdDiff<-r_squared_combined - r_squared_bpd\r\ncat(\"There is a\",bpdDiff,\"increase in r squared\r\nFrom\",r_squared_bpd,\"to\",r_squared_combined)\r\n\r\nThere is a 0.1362216 increase in r squared\r\nFrom 0.7220601 to 0.8582817\r\n\r\n# Both cases indicate a higher R Squared value from the combined model.\r\n\r\n# B3. Just an additional question (This will not be graded). \r\n# When should we consider \"log-transforming\" a dataset? \r\n# This is a very common practice in data science. \r\n\r\ncat(\"We should consider 'log-transforming' a dataset when our data is skewed,\r\nhas multiplicative relationships that are modeled with linear regression,\r\nand where the variance of the data is not constant in the range of values.\")\r\n\r\nWe should consider 'log-transforming' a dataset when our data is skewed,\r\nhas multiplicative relationships that are modeled with linear regression,\r\nand where the variance of the data is not constant in the range of values.\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/multivariate.png?raw=true",
    "last_modified": "2025-03-26T10:50:44-04:00",
    "input_file": "module-10-assignment.knit.md"
  },
  {
    "path": "posts/2025-03-19-module-9-assignment/",
    "title": "Module-9-Assignment",
    "description": "Introduction to tabular data and contingency tables",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-03-19",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 3/19/2025\r\n\r\n# 'Question A'\r\n\r\n# Your data.frame is listed as below.\r\n\r\n# Create the data frame with country, age, salary, and purchased variables\r\ndf <- data.frame(\r\n  country = c(\"France\", \"Spain\", \"Germany\", \"Spain\", \"Germany\", \"France\", \"Spain\", \"France\", \"Germany\", \"France\"),\r\n  age = c(44, 27, 30, 38, 40, 35, 52, 48, 45, 37),\r\n  salary = c(6000, 5000, 7000, 4000, 8000, 6000, 5000, 7000, 4000, 8000),\r\n  purchased = c(\"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\")\r\n)\r\n\r\n# A1. Generate a one-way table for \"purchased\"\r\ntableOne <- table(df$purchased)\r\ntableOne\r\n\r\n\r\n No Yes \r\n  5   5 \r\n\r\n# The table() function counts the frequency \"Yes\" and \"No\"\r\n# appear in the \"purchased\" column of the data frame.\r\n\r\n# The results show that five did not purchase, and\r\n# five did purchase.\r\n\r\n# A2. Generate a two-way table for \"country\" and \"purchased.\"\r\ntableTwo <- table(df$country, df$purchased)\r\ntableTwo\r\n\r\n         \r\n          No Yes\r\n  France   1   3\r\n  Germany  2   1\r\n  Spain    2   1\r\n\r\n# This is generating a table that will measure the relationship \r\n# between \"country\" variables and \"purchased\" variables\r\n\r\n# Ex. In \"Germany,\" two said \"No\" to purchasing, while one said \"Yes.\"\r\n\r\n\r\n\r\n# 'Question B'\r\n\r\n# We will analyze a data frame by creating a contingency table based on the\r\n# number of cylinders and number of gears.\r\n\r\n# Generate contingency table also known as rx C table using mtcars dataset. \r\ndata(mtcars)\r\nmtcars_df <- table(mtcars$gear, mtcars$cyl, dnn = c(\"Gears\", \"Cylinders\"))\r\n\r\n# The table counts how many cars in the dataframe have a certain number of\r\n# cylinders and gears.\r\n\r\n\r\n# B1. Add the addmargins() function to report on the sum totals of \r\n# the rows and columns of \"mtcars_df\" table\r\nmtcarsTableMargins <- addmargins(mtcars_df)\r\nmtcarsTableMargins\r\n\r\n     Cylinders\r\nGears  4  6  8 Sum\r\n  3    1  2 12  15\r\n  4    8  4  0  12\r\n  5    2  1  2   5\r\n  Sum 11  7 14  32\r\n\r\n# The addMargins() function adds row and column sums to the table, helping\r\n# me see the total number of counts.\r\n\r\n# What is displayed includes an extra row and column that is labeled \"Sum,\"\r\n# which indicates the total number of values in a particular row or column.\r\n\r\n\r\n# B2. Add prop.table() function, and report on the proportional weight \r\n# of each value in the \"mtcars_df\" table\r\nmtcarsTableProp <- prop.table(mtcars_df)\r\nmtcarsTableProp\r\n\r\n     Cylinders\r\nGears       4       6       8\r\n    3 0.03125 0.06250 0.37500\r\n    4 0.25000 0.12500 0.00000\r\n    5 0.06250 0.03125 0.06250\r\n\r\n# The prop.table() function converts the table into proportions. This helps\r\n# show the percentage of each category that is compared to the total number\r\n# of observations.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/tabulardata.png?raw=true",
    "last_modified": "2025-03-19T13:29:56-04:00",
    "input_file": "module-9-assignment.knit.md"
  },
  {
    "path": "posts/2025-03-06-module-8-assignment/",
    "title": "Module-8-Assignment",
    "description": "Introduction to ANOVA",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-03-06",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 3/6/2025\r\n\r\n# 'Question A'\r\n\r\n# A researcher is interested in the effects of drug against stress reactions. \r\n# She gives a reaction time test to three different groups of subjects: \r\n# one group that is under a great deal of stress, one group under a \r\n# moderate amount of stress, and a third group that is under almost no stress. \r\n# The subjects of the study were instructed to take the drug test during \r\n# their next stress episode and to report their stress on a scale of 1 to 10 \r\n# (10 being the most pain).\r\n\r\n\r\n\r\n\r\n# Report on drug and stress level by using R. Provide a full summary report \r\n# on the result of ANOVA testing and what it means.\r\n# More specifically, report the following: \r\n# Df, Sum, Sq Mean, Sq, F value, Pr(>F)\r\n\r\n# Create vectors for each group in stress data table\r\nhighStress <- c(10, 9, 8, 9, 10, 8)\r\nmoderateStress <- c(8, 10, 6, 7, 8, 8)\r\nlowStress <- c(4, 6, 6, 4, 2, 2)\r\n\r\n# Combine the data into a single response variable\r\nstressLevels <- c(highStress, moderateStress, lowStress)\r\n\r\n# Create a data frame for the values and stress level factors\r\nstressData <- data.frame(value = c(stressLevels),\r\nstressAmounts = factor(rep(c(\"High\", \"Moderate\", \"Low\"), each = 6)))\r\n\r\n# ANOVA test\r\nanovaStressResults <- aov(value ~ stressAmounts, data = stressData)\r\n\r\n# Create a summary of the results\r\nsummary(anovaStressResults)\r\n\r\n              Df Sum Sq Mean Sq F value   Pr(>F)    \r\nstressAmounts  2  82.11   41.06   21.36 4.08e-05 ***\r\nResiduals     15  28.83    1.92                     \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n# Full summary report on the result of ANOVA testing:\r\n\r\n# Df for stressAmounts is 2. This stands for k - 1, which is the\r\n# number of samples minus 1, which equals 2.\r\n# Df for the residuals is 15. This is N - k, where N equals the\r\n# number of observations and k equals the number of samples.\r\n# If you add these together, they equal n - 1, total df.\r\n\r\n# Sum sq stands for SS(B) + SS(W), where SS(B) = 82.11, which represents\r\n# the sum of squares for between-treatment variance and \r\n# SS(W) = 28.83, which represents the sum of squares for\r\n# within-treatment variance. Adding SS(B) + SS(W) gives the total variability.\r\n\r\n# Mean Sq for stressAmounts is the mean of the sum of squares of the groups \r\n# that receive the treatment, which equals 41.06, and mean sq for the residuals\r\n# is the mean of the sum of squares of the groups without treatment effects,\r\n# which equals 1.92. Together, they equal the F ratio, MS(B)/MS(W).\r\n\r\n# The F value is 21.36. An F value of 1 would indicate no difference between\r\n# the sample means MS(B) and the sample variations MS(W). This large F value\r\n# indicates strong evidence that there is a difference between the group means.\r\n\r\n# The Pr(>F) value is 4.08e-05 ***, which is a very low p-value, less than\r\n# 0.05. Therefore, we reject the null hypothesis that there are no differences\r\n# between the means. In this case, we can conclude that there are significant\r\n# differences within the group means.\r\n\r\n# 'Question B'\r\n\r\n# The zelazo data (taken from the textbook's R package called ISwR) are in \r\n# the form of a list of vectors, one for each of the four groups.\r\n\r\n# B1. Convert the data to a form suitable for the user of lm, and calculate \r\n# the relevant test. Consider t-tests comparing selected subgroups or \r\n# obtained by combining groups.\r\n\r\n# Load the data set\r\nlibrary(ISwR)\r\ndata(\"zelazo\")\r\n\r\n# Convert the data to a form suitable for the user\r\nzelazoDf <- data.frame(\r\nzelazoValue = c(zelazo$active, zelazo$passive, zelazo$none, zelazo$ctr.8w),\r\nzelazoTraining = factor(rep(c(\"Active\", \"Passive\", \"None\", \"Control\"),\r\ntimes = c(length(zelazo$active), length(zelazo$passive), length(zelazo$none),\r\n          length(zelazo$ctr.8w)))))\r\n\r\n# Calculate the relevant linear model (lm) test\r\nlinearModel <- lm(zelazoDf$zelazoValue~zelazoDf$zelazoTraining,data=zelazoDf)\r\nsummary(linearModel)\r\n\r\n\r\nCall:\r\nlm(formula = zelazoDf$zelazoValue ~ zelazoDf$zelazoTraining, \r\n    data = zelazoDf)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.7083 -0.8500 -0.3500  0.6375  3.6250 \r\n\r\nCoefficients:\r\n                               Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)                     10.1250     0.6191  16.355 1.19e-12\r\nzelazoDf$zelazoTrainingControl   2.2250     0.9182   2.423   0.0255\r\nzelazoDf$zelazoTrainingNone      1.5833     0.8755   1.809   0.0864\r\nzelazoDf$zelazoTrainingPassive   1.2500     0.8755   1.428   0.1696\r\n                                  \r\n(Intercept)                    ***\r\nzelazoDf$zelazoTrainingControl *  \r\nzelazoDf$zelazoTrainingNone    .  \r\nzelazoDf$zelazoTrainingPassive    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.516 on 19 degrees of freedom\r\nMultiple R-squared:  0.2528,    Adjusted R-squared:  0.1348 \r\nF-statistic: 2.142 on 3 and 19 DF,  p-value: 0.1285\r\n\r\n# Consider t-tests comparing selected subgroups:\r\n\r\n# t-test for active training versus no training\r\nactiveVersusNone <- t.test(zelazo$active, zelazo$none)\r\nactiveVersusNone\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  zelazo$active and zelazo$none\r\nt = -1.8481, df = 9.9759, p-value = 0.09442\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -3.4929271  0.3262604\r\nsample estimates:\r\nmean of x mean of y \r\n 10.12500  11.70833 \r\n\r\n# The p-value of 0.09442 is greater than 0.05, so there is no significant\r\n# evidence to reject H0 and there is no difference between the means\r\n\r\n\r\n# t-test for active training versus passive training\r\nactiveVersusPassive <- t.test(zelazo$active, zelazo$passive)\r\nactiveVersusPassive\r\n\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  zelazo$active and zelazo$passive\r\nt = -1.2839, df = 9.3497, p-value = 0.2301\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -3.4399668  0.9399668\r\nsample estimates:\r\nmean of x mean of y \r\n   10.125    11.375 \r\n\r\n# The p-value of 0.2301 is greater than 0.05, so there is no significant\r\n# evidence to reject H0 and there is no difference between the means\r\n\r\n# t-test for active training versus control training\r\nactiveVersusControl <- t.test(zelazo$active, zelazo$control)\r\nactiveVersusControl\r\n\r\n\r\n    One Sample t-test\r\n\r\ndata:  zelazo$active\r\nt = 17.14, df = 5, p-value = 1.238e-05\r\nalternative hypothesis: true mean is not equal to 0\r\n95 percent confidence interval:\r\n  8.606488 11.643512\r\nsample estimates:\r\nmean of x \r\n   10.125 \r\n\r\n# The p-value 1.238e-05 is less than 0.05, so there is significant evidence\r\n# to reject H0 and a difference does exist between these two groups\r\n\r\n# B2. Consider the ANOVA test (one-way or two-way) for this dataset (zelazo)\r\n\r\n# Perform the one-way test for this data set using the oneway.test() function\r\noneway.test(zelazoDf$zelazoValue~zelazoDf$zelazoTraining)\r\n\r\n\r\n    One-way analysis of means (not assuming equal variances)\r\n\r\ndata:  zelazoDf$zelazoValue and zelazoDf$zelazoTraining\r\nF = 2.7759, num df = 3.000, denom df = 10.506, p-value =\r\n0.09373\r\n\r\n# The oneway.test() function provides information on the F value, the p-value,\r\n# and the df for both the numerator and the denominator.\r\n\r\n# Perform the ANOVA test for this data set using the aov() function\r\nzelazoAOV <- aov(zelazoDf$zelazoValue~zelazoDf$zelazoTraining, data=zelazoDf)\r\nsummary(zelazoAOV)\r\n\r\n                        Df Sum Sq Mean Sq F value Pr(>F)\r\nzelazoDf$zelazoTraining  3  14.78   4.926   2.142  0.129\r\nResiduals               19  43.69   2.299               \r\n\r\n# After using the aov() function, I then called the summary() function so that\r\n# I can print a table to get the variations, zelazo training group values, \r\n# and residual values.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/ANOVA.png?raw=true",
    "last_modified": "2025-03-06T15:19:45-05:00",
    "input_file": "module-8-assignment.knit.md"
  },
  {
    "path": "posts/2025-02-25-module-7-assignment/",
    "title": "Module-7-Assignment",
    "description": "Simple Linear and Multiple Regression Analysis",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-02-28",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 2/28/2025\r\n\r\n# 'Question A'\r\n\r\n# In this assignment's segment, we will use the following regression equation\r\n# Y = a + bX + e\r\n\r\n# -> Y is the value of the dependent variable (Y), what is being \r\n# predicted or explained\r\n\r\n# -> a or Alpha, a constant; equals the value of Y when the value of X=0\r\n\r\n# -> b or Beta, the coefficient of X; the slope of the regression line; \r\n# how much Y changes for each one-unit change in X.\r\n\r\n# -> X is the value of the Independent variable (X), what is predicting \r\n# or explaining the value of Y\r\n\r\n# -> e is the error term; the error in predicting the value of Y, given \r\n# the value of X (it is not displayed in most regression equations).\r\n\r\n# The data in this assignment:\r\n\r\nx <- c(16, 17, 13, 18, 12, 14, 19, 11, 11, 10)\r\ny <- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)\r\n\r\n# A1. Define the relationship model between the independent and\r\n# the dependent variable.\r\n\r\n# Define the relationship model by creating a linear model\r\nrelModel <- lm(y ~ x)\r\nrelModel\r\n\r\n\r\nCall:\r\nlm(formula = y ~ x)\r\n\r\nCoefficients:\r\n(Intercept)            x  \r\n     19.206        3.269  \r\n\r\n# Print a summary of the regression model\r\nsummary(relModel)\r\n\r\n\r\nCall:\r\nlm(formula = y ~ x)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-11.435  -7.406  -4.608   6.681  16.834 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept)   19.206     15.691   1.224   0.2558  \r\nx              3.269      1.088   3.006   0.0169 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 10.48 on 8 degrees of freedom\r\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.4716 \r\nF-statistic: 9.033 on 1 and 8 DF,  p-value: 0.01693\r\n\r\n# A2. Calculate the coefficients\r\nrelModelCoeffs <- coefficients(relModel)\r\n# The coefficients calculated will be displayed as the\r\n# values of b0 (the y-intercept) and b1 (the slope)\r\n\r\n# b0 represents the intercept or constant and is the first coefficient\r\n# It represents the y-value when x = 0\r\nrelModelCoeffs[1]\r\n\r\n(Intercept) \r\n    19.2056 \r\n\r\n# b1 represents the slope and is the second, regression coefficient.\r\n# It represents how much y changes as x changes.\r\nrelModelCoeffs[2]\r\n\r\n       x \r\n3.269107 \r\n\r\n# 'Question B'\r\n\r\n# The following question is posted by Chi Yau the author of R Tutorial With \r\n# Bayesian Statistics Using Stan and his blog posting regarding Regression \r\n# analysis.\r\n\r\n# Apply the simple linear regression model (see the above formula) for the \r\n# data set called \"visit\" (see below), and estimate the discharge duration \r\n# if the waiting time since the last eruption has been 80 minutes.\r\n\r\n# > head(visit)\r\n\r\n# discharge  waiting \r\n# 1   3.600      79 \r\n# 2   1.800      54 \r\n# 3   3.333      74 \r\n# 4   2.283      62 \r\n# 5   4.533      85 \r\n# 6   2.883      55\r\n\r\nvisit <- data.frame(\"discharge\" = c(3.600, 1.800, 3.333, 2.283, 4.533, 2.883),\r\n                    \"waiting\"=c(79, 54, 74, 62, 85, 55))\r\n\r\n# Employ the following formula discharge ~ waiting and data=visit)\r\n\r\n# B1. Define the relationship model between the predictor \r\n# and the response variable.\r\n\r\n# The response variable y is \"discharge\" and is predicted by\r\n# the predictor variable x, which is \"waiting\"\r\n\r\n# Model the relationship between the predictor variable x and \r\n# the response variable y\r\ndischarge <- lm(discharge ~ waiting, data=visit)\r\ndischarge\r\n\r\n\r\nCall:\r\nlm(formula = discharge ~ waiting, data = visit)\r\n\r\nCoefficients:\r\n(Intercept)      waiting  \r\n   -1.53317      0.06756  \r\n\r\n# B2. Extract the parameters of the estimated regression equation with \r\n# the coefficients function.\r\ncoeff <- coefficients(discharge)\r\ncoeff\r\n\r\n(Intercept)     waiting \r\n-1.53317418  0.06755757 \r\n\r\n# B3. Determine the fit of the eruption duration using \r\n# the estimated regression equation.\r\n\r\n# Here is the value for the waiting time since the last eruption, also\r\n# known as x (named \"waiting\").\r\nwaiting <- 80\r\n\r\n# Use the estimated regression equation to fit the discharge duration\r\npredictedDis <- coeff[1]+coeff[2]*waiting\r\npredictedDis\r\n\r\n(Intercept) \r\n   3.871431 \r\n\r\n# Based on the simple linear regression model, if the waiting time since\r\n# the last discharge has been 80 minutes, we expect the next discharge\r\n# To last for 3.87 minutes.\r\n\r\n# 'Question C. Multiple Regression'\r\n\r\n# We will use a very famous dataset in R called mtcars. This dataset \r\n# was extracted from the 1974 Motor Trend US magazine, and comprises \r\n# fuel consumption and 10 aspects of automobile design and performance \r\n# for 32 automobiles (1973--74 models).\r\n\r\n# This data frame contain 32 observations on 11 (numeric) variables.\r\n\r\n# [, 1]  mpg  Miles/(US) gallon\r\n# [, 2]  cyl  Number of cylinders\r\n# [, 3]  disp  Displacement (cu.in.)\r\n# [, 4]  hp  Gross horsepower\r\n# [, 5]  drat  Rear axle ratio\r\n# [, 6]  wt  Weight (1000 lbs)\r\n# [, 7]  qsec  1/4 mile time\r\n# [, 8]  vs  Engine (0 = V-shaped, 1 = straight)\r\n# [, 9]  am  Transmission (0 = automatic, 1 = manual)\r\n# [,10]  gear  Number of forward gears\r\n\r\n# To call mtcars data in R\r\n# R comes with several built-in data sets, which are generally used as \r\n# demo data for playing with R functions. One of those datasets build \r\n# in R is mtcars. In this question, we will use 4 of the variables found \r\n# in mtcars by using the following function\r\n\r\n#input <- mtcars[,c(\"mpg\",\"disp\",\"hp\",\"wt\")]\r\n#print(head(input))\r\n\r\n# C1. Examine the relationship Multi Regression Model as stated above \r\n# and its Coefficients using 4 different variables \r\n# from mtcars (mpg, disp, hp and wt). Report on the result and explanation \r\n# what does the multi regression model and coefficients tell about the data. \r\n\r\n# Claim the mtcars dataset by loading it\r\ndata(mtcars)\r\n\r\n# Select the variables that are of importance\r\ninput <- mtcars[,c(\"mpg\", \"disp\", \"hp\", \"wt\")]\r\nprint(head(input))\r\n\r\n                   mpg disp  hp    wt\r\nMazda RX4         21.0  160 110 2.620\r\nMazda RX4 Wag     21.0  160 110 2.875\r\nDatsun 710        22.8  108  93 2.320\r\nHornet 4 Drive    21.4  258 110 3.215\r\nHornet Sportabout 18.7  360 175 3.440\r\nValiant           18.1  225 105 3.460\r\n\r\n# Apply the multiple linear regression equation\r\nmultModel <- lm(formula = mpg ~ disp + hp + wt, data = input)\r\n\r\n# Print a summary of the regression model\r\nsummary(multModel)\r\n\r\n\r\nCall:\r\nlm(formula = mpg ~ disp + hp + wt, data = input)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-3.891 -1.640 -0.172  1.061  5.861 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 37.105505   2.110815  17.579  < 2e-16 ***\r\ndisp        -0.000937   0.010350  -0.091  0.92851    \r\nhp          -0.031157   0.011436  -2.724  0.01097 *  \r\nwt          -3.800891   1.066191  -3.565  0.00133 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2.639 on 28 degrees of freedom\r\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8083 \r\nF-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11\r\n\r\n# Calculate the coefficients\r\ncoefficients(multModel)\r\n\r\n  (Intercept)          disp            hp            wt \r\n37.1055052690 -0.0009370091 -0.0311565508 -3.8008905826 \r\n\r\n# The main objective of the multiple regression model that was used is to\r\n# analyze the relationship between the fuel efficiency (variable mpg)\r\n# and the three independent variables: disp for displacement, hp \r\n# for horse power, and wt for weight. Using the data set called mtcars,\r\n# I was able to create a multiple regression model to estimate the\r\n# mpg based on the three variables. According to the output from the multModel\r\n# variable, wt is the strongest predictor for fuel efficiency. In summary, as \r\n# the weight gets larger, the miles per gallon gets smaller. Also, as the miles\r\n# per gallon increases, the weight decreases. With this in mind, every\r\n# predictor variable affects the miles per gallon differently.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/regression.png?raw=true",
    "last_modified": "2025-03-06T15:19:25-05:00",
    "input_file": "module-7-assignment.knit.md"
  },
  {
    "path": "posts/2025-02-18-module-6-assignment/",
    "title": "Module-6-Assignment",
    "description": "Random Variable(s) and Probability Distribution(s)",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-02-18",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 2/18/2025\r\n\r\n# \"Question A\"\r\n\r\n# Consider a population consisting of the following values, \r\n# which represents the number of ice cream purchases during \r\n# one year for each of the five housemates.8, 14, 16, 10, 11.\r\n\r\n# Here are the population values\r\npopVal <- c(8, 14, 16, 10, 11)\r\npopVal\r\n\r\n[1]  8 14 16 10 11\r\n\r\n# A1. Compute the mean of this population.\r\nmu <- mean(popVal)\r\nmu\r\n\r\n[1] 11.8\r\n\r\n# A2. Select a random sample of size 2 out of the five members.\r\nsampVal <- sample(popVal, size = 2, replace = FALSE)\r\nsampVal\r\n\r\n[1] 11 14\r\n\r\n# A3. Compute the mean and standard deviation of your sample.\r\nxBar <- mean(sampVal)\r\nsampleStdDev <- sd(sampVal)\r\nxBar\r\n\r\n[1] 12.5\r\n\r\nsampleStdDev\r\n\r\n[1] 2.12132\r\n\r\n# A4. Compare the Mean and Standard deviation of your sample \r\n# to the entire population of this set (8,14, 16, 10, 11).\r\n\r\n# Calculate population standard deviation\r\npopStdDev <- sd(popVal)\r\n\r\n# The mean and the standard deviation of the sample are:\r\npaste(\"Sample Mean:\",xBar, \"/ Sample Standard Deviation:\",sampleStdDev)\r\n\r\n[1] \"Sample Mean: 12.5 / Sample Standard Deviation: 2.12132034355964\"\r\n\r\n# The mean and the standard deviation of the population are:\r\npaste(\"Population Mean:\",mu, \"/ Population Standard Deviation:\",popStdDev)\r\n\r\n[1] \"Population Mean: 11.8 / Population Standard Deviation: 3.19374388453426\"\r\n\r\n# If the sample includes values close to the population mean,\r\n# the sample mean will be similar to the population mean\r\n\r\n# If the sample has outliers, the sample mean might be further\r\n# from the population mean\r\n\r\n# If the standard deviation of the sample is greater than the\r\n# standard deviation of the population, the data will have a\r\n# wider spread across the x-axis\r\n\r\n\r\n# \"Question B\"\r\n# Suppose that the sample size n = 100 and the population proportion p = 0.95.\r\n\r\n\r\n# B1. Does the sample proportion p have approximately a \r\n# normal distribution? Explain.\r\n\r\n# The distribution is expected to be normal if both n*p and n*q are greater\r\n# than or equal to 5 \r\n\r\n# Sample size\r\nn <- 100\r\n\r\n# Probability of success\r\np <- 0.95\r\n\r\n# Probability of failure\r\nq <- 1 - p\r\n\r\n# Apply the rules\r\nnp <- n * p\r\nnq <- n * q\r\nnp\r\n\r\n[1] 95\r\n\r\nnq\r\n\r\n[1] 5\r\n\r\n# Since p = .95, q = .05. p * n = .95 * 100 = 95 \r\n# and q * n = .05 * 100 = 5, I can conclude that the sampling proportion p \r\n# will have approximately a normal distribution.\r\n\r\n\r\n# B2. What is the smallest value of n for which the sampling \r\n# distribution of p is approximately normal?\r\n\r\ncat(\"The smallest value of n for which the sampling distribution is\r\napproximately normal is 100. Any value for n less than 100\r\nwill make n*q less than 5. In order for the sampling \r\ndistribution of p to be considered approximately normal,\r\nboth n*p and n*q must be greater than or equal to 5.\r\nTherefore, n = 100 is the smallest value.\")\r\n\r\nThe smallest value of n for which the sampling distribution is\r\napproximately normal is 100. Any value for n less than 100\r\nwill make n*q less than 5. In order for the sampling \r\ndistribution of p to be considered approximately normal,\r\nboth n*p and n*q must be greater than or equal to 5.\r\nTherefore, n = 100 is the smallest value.\r\n\r\n# \"Question C\"\r\n# From our textbook, Chapter 2 Probability Exercises # 2.4. \r\n# Simulated coin tossing is probability better done using \r\n# function called rbinom than using the function called sample.\r\n\r\n# C1. Please explain the reason why rbinom is better than \r\n# sample in the coin tossing simulation.\r\n\r\ncat(\"ribnom is better than the function sample for a coin toss\r\nbecause rbinom is specifically modeled to generate random\r\nsamples from a binomial distribution, which models the exact\r\nprobability of getting heads or tails in a coin toss experiment.\r\nSample requires creating a vector with \\\"Heads\\\" or \\\"Tails\\\"\r\nelements and then randomly sampling from that vector. This extra\r\nstep is unnecessary with rbinom because it is programmed to generate\r\nthe binomial experiment outcome based on the binomial distribution.\r\nrbinom is a good choice for determining the number of successes\r\nand failures in a fixed number of independent trials.\r\nOverall, it is the most accurate in terms of events that\r\ndeal with one of two possible outcomes. In this case\r\nwith the coin flip, the two possible outcomes would be\r\nheads or tails. \")\r\n\r\nribnom is better than the function sample for a coin toss\r\nbecause rbinom is specifically modeled to generate random\r\nsamples from a binomial distribution, which models the exact\r\nprobability of getting heads or tails in a coin toss experiment.\r\nSample requires creating a vector with \"Heads\" or \"Tails\"\r\nelements and then randomly sampling from that vector. This extra\r\nstep is unnecessary with rbinom because it is programmed to generate\r\nthe binomial experiment outcome based on the binomial distribution.\r\nrbinom is a good choice for determining the number of successes\r\nand failures in a fixed number of independent trials.\r\nOverall, it is the most accurate in terms of events that\r\ndeal with one of two possible outcomes. In this case\r\nwith the coin flip, the two possible outcomes would be\r\nheads or tails. \r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/Random-Variable.jpg?raw=true",
    "last_modified": "2025-02-18T14:41:34-05:00",
    "input_file": "module-6-assignment.knit.md"
  },
  {
    "path": "posts/2025-02-11-module-5-assignment/",
    "title": "Module-5-Assignment",
    "description": "Hypothesis testing and correlation analysis",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-02-15",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 2/15/2025\r\n\r\n# \"Question A\"\r\n\r\n# The director of manufacturing at a cookies company needs to determine \r\n# whether a new machine is able to produce a particular type of cookies \r\n# according to the manufacturer's specifications, which indicate that \r\n# cookies should have a mean of 70 and standard deviation of 3.5 pounds. \r\n# A sample of 49 cookies reveals a sample mean breaking strength of 69.1 pounds.\r\n\r\n# A1. State the null and alternative hypothesis\r\n\r\n# \"The null hypothesis (H0) would be that the breaking strength of the cookies\r\n# is equal to 70 pounds.\"\r\n\r\n# \"The alternative hypothesis (Ha) would be that the breaking strength of\r\n# the cookies is not equal to 70 pounds.\"\r\n\r\n# A2. Is there evidence that the machine is not meeting the manufacturer's \r\n# specifications for average strength? Use a 0.05 level of significance\r\n\r\n# Here are the sample statistics we are dealing with\r\nsSize <- 49\r\nsMean <- 69.1\r\npStdDev <- 3.5\r\n\r\n# Here is the level of significance that will be used\r\na <- 0.05\r\n\r\n# Here is the population mean\r\nmu <- 70\r\n\r\n# Calculate the z-score\r\nz <- (sMean - mu) / (pStdDev/sqrt(sSize))\r\nz\r\n\r\n[1] -1.8\r\n\r\n# A3. Compute the p value and interpret its meaning\r\n\r\n\r\n# Calculate the p-value. This is a two-tailed test\r\npVal <- 2 * pnorm(abs(z), lower.tail = FALSE)\r\npVal\r\n\r\n[1] 0.07186064\r\n\r\n# Make the interpretation\r\nif (pVal < a) {\r\n  cat(\"Reject H0. There is enough evidence to suggest that the machine \r\n      is not meeting the manufacturer's specifications\r\n      for the average strength\")\r\n} else {\r\n  cat(\"Fail to reject H0. There is not enough evidence to suggest that\r\n       the machine is not meeting the manufacturer's specifications \r\n       for the average strength\")\r\n}\r\n\r\nFail to reject H0. There is not enough evidence to suggest that\r\n       the machine is not meeting the manufacturer's specifications \r\n       for the average strength\r\n\r\n# A4. What would be your answer in (B) if the standard deviation were \r\n# specified as 1.75 pounds?\r\n\r\n# The population standard deviation is now 1.75\r\npStdDev <- 1.75\r\n\r\n# Calculate the z-score\r\nz <- (sMean - mu) / (pStdDev/sqrt(sSize))\r\nz\r\n\r\n[1] -3.6\r\n\r\n# Calculate the p-value. This is a two-tailed test\r\npVal <- 2 * pnorm(abs(z), lower.tail = FALSE)\r\npVal\r\n\r\n[1] 0.0003182172\r\n\r\n# Make the interpretation\r\nif (pVal < a) {\r\n  cat(\"Reject H0. There is enough evidence to suggest that the machine \r\n      is not meeting the manufacturer's specifications\r\n      for the average strength\")\r\n} else {\r\n  cat(\"Fail to reject H0. There is not enough evidence to suggest that\r\n       the machine is not meeting the manufacturer's specifications \r\n       for the average strength\")\r\n}\r\n\r\nReject H0. There is enough evidence to suggest that the machine \r\n      is not meeting the manufacturer's specifications\r\n      for the average strength\r\n\r\n# A5. What would be your answer in (B) if the sample mean were 69 pounds \r\n# and the standard deviation is 3.5 pounds?\r\n\r\n# The sample mean is now 69\r\nsMean <- 69\r\n\r\n# The population standard deviation is now 3.5\r\npStdDev <- 3.5\r\n\r\n# Calculate the z-score\r\nz <- (sMean - mu) / (pStdDev/sqrt(sSize))\r\nz\r\n\r\n[1] -2\r\n\r\n# Calculate the p-value. This is a two-tailed test\r\npVal <- 2 * pnorm(abs(z), lower.tail = FALSE)\r\npVal\r\n\r\n[1] 0.04550026\r\n\r\n# Make the interpretation\r\nif (pVal < a) {\r\n  cat(\"Reject H0. There is enough evidence to suggest that the machine \r\n      is not meeting the manufacturer's specifications\r\n      for the average strength\")\r\n} else {\r\n  cat(\"Fail to reject H0. There is not enough evidence to suggest that\r\n       the machine is not meeting the manufacturer's specifications \r\n       for the average strength\")\r\n}\r\n\r\nReject H0. There is enough evidence to suggest that the machine \r\n      is not meeting the manufacturer's specifications\r\n      for the average strength\r\n\r\n# \"Question B\"\r\n\r\n# If xbar = 85, population standard deviation = 8, and n = 64, set up\r\n# 95% confidence interval estimate of the population mean mu.\r\n\r\n# List the set of values given\r\nsMean <- 85\r\npStdDev <- 8\r\nsSize <- 64\r\nconLevel <- 0.95\r\n\r\n# Calculate the z-score for the given confidence level\r\na <- 1 - conLevel\r\na\r\n\r\n[1] 0.05\r\n\r\n# This is a two-tailed test, so alpha will be cut in half\r\nz_crit <- qnorm(1 - a/2)\r\nz_crit\r\n\r\n[1] 1.959964\r\n\r\n# Solve for e, which stands for the margin of error\r\ne <- z_crit * (pStdDev / sqrt(sSize))\r\ne\r\n\r\n[1] 1.959964\r\n\r\n# Calculate the upper and lower limit to set up the\r\n# 95% confidence interval\r\n\r\nlower_limit <- sMean - e\r\nlower_limit\r\n\r\n[1] 83.04004\r\n\r\nupper_limit <- sMean + e\r\nupper_limit\r\n\r\n[1] 86.95996\r\n\r\n# Confidence interval\r\nprint(\"(83.04, 86.96)\")\r\n\r\n[1] \"(83.04, 86.96)\"\r\n\r\n# There is a 95% probability that the population mean would be\r\n# in between these two values\r\n\r\n\r\n# \"Question C\"\r\nlibrary(GGally)\r\n# Given the time spent on assignments each week for the sampled \r\n# girl group and boy group, respectively,\r\n\r\ngirls_grades <- c(89, 90, 91, 95, 98, 99, 96, 99)\r\ngirls_time_spend <- c(19, 20, 22, 25, 28, 30, 32, 36)\r\n\r\nboys_grades <- c(86, 84, 92, 93, 93, 96, 98, 98)\r\nboys_time_spend <- c(15, 19, 22, 23, 25, 29, 30, 40)\r\n\r\n# Please perform the correlation analysis:\r\n\r\n# C1. Calculate the correlation coefficient (Pearson) between time \r\n# spent and grade for girls' and boys' data sets, respectively.\r\n\r\n# Here is the Pearson correlation for the girls\r\nx <- (girls_grades)\r\ny <- (girls_time_spend)\r\ngirlsCorr <- data.frame(x, y)\r\ncor(girlsCorr, method = \"pearson\")\r\n\r\n         x        y\r\nx 1.000000 0.906671\r\ny 0.906671 1.000000\r\n\r\n# Here is the Pearson correlation for the boys\r\nx <- (boys_grades)\r\ny <- (boys_time_spend)\r\nboysCorr <- data.frame(x, y)\r\ncor(boysCorr, method = \"pearson\")\r\n\r\n          x         y\r\nx 1.0000000 0.8621872\r\ny 0.8621872 1.0000000\r\n\r\n# C2. Use ggpairs to plot the time spent and grade for \r\n# girls' and boys' datasets, respectively\r\n\r\n# Here is the plot to compare the time spent\r\n# and grade for the girls\r\nggpairs(girlsCorr, upper = list(wrap(\"cor\", size = 5)),\r\ndiag = list(continuous = \"densityDiag\"),)\r\n\r\n\r\n# Here is the plot to compare the time spent\r\n# and grade for the boys\r\nggpairs(boysCorr, upper = list(wrap(\"cor\", size = 5)),\r\ndiag = list(continuous = \"densityDiag\"),)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/hypotest.png?raw=true",
    "last_modified": "2025-02-15T20:48:36-05:00",
    "input_file": "module-5-assignment.knit.md"
  },
  {
    "path": "posts/2025-02-08-module-4-assignment/",
    "title": "Module-4-Assignment",
    "description": "Probability: Bayes' Theorem.",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-02-08",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 2/8/2025\r\n\r\n# \"Question A\"\r\n# Based on Table 1, what is the probability of\r\n\r\n# A1. Event A\r\nA <- 30/90\r\nA\r\n\r\n[1] 0.3333333\r\n\r\n# A2. Event B\r\nB <- 30/90\r\nB\r\n\r\n[1] 0.3333333\r\n\r\n# A3. Event A or B\r\nAUB <- A + B - (10/90)\r\nAUB\r\n\r\n[1] 0.5555556\r\n\r\n# A4. P(A or B) = P(A) + P(B)\r\nAUB == A + B\r\n\r\n[1] FALSE\r\n\r\n# \"Question B\"\r\n# B1. Is this answer True or False.\r\nprint(\"TRUE\")\r\n\r\n[1] \"TRUE\"\r\n\r\n# Please explain why\r\ncat(\"The answer is True because we determined that\r\n    there is a low initial probability of an event occurring (A1) before\r\n    the weather man made the report (B). This explains why even a seemingly \r\n    accurate prediction can be misleading if the probability of an event \r\n    is very low. If it only normally rains 5 out of 365\r\n    days, there is a historically low probability for rain. Even after\r\n    learning the new information, it does not significantly\r\n    change the prior belief enough for it to become very likely.\r\n    If rain were to occur on more days, then the weather mans\r\n    prediction would be more likely to happen. Bayes' Theorem\r\n    still indicates a good chance of not raining.\")\r\n\r\nThe answer is True because we determined that\r\n    there is a low initial probability of an event occurring (A1) before\r\n    the weather man made the report (B). This explains why even a seemingly \r\n    accurate prediction can be misleading if the probability of an event \r\n    is very low. If it only normally rains 5 out of 365\r\n    days, there is a historically low probability for rain. Even after\r\n    learning the new information, it does not significantly\r\n    change the prior belief enough for it to become very likely.\r\n    If rain were to occur on more days, then the weather mans\r\n    prediction would be more likely to happen. Bayes' Theorem\r\n    still indicates a good chance of not raining.\r\n\r\n# \"Question C\"\r\n# For a disease known to have a postoperative complication frequency of 20%, \r\n# a surgeon suggests a new procedure.She/he tests it on 10 patients and found \r\n# there are not complications. \r\n\r\n# C1. What is the probability of operating on 10 patients successfully with the\r\n# traditional method?\r\n\r\n# List the required parameters\r\nnumPatients <- 10\r\nsuccessPer <- 0.8\r\nnumSuccess <- 10\r\n\r\n# Solve problem with the dbinom function\r\nprobOfSuccess <- dbinom(numSuccess, size = numPatients, prob = successPer)\r\n# Print the probability\r\nprobOfSuccess\r\n\r\n[1] 0.1073742\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/probability.jpg?raw=true",
    "last_modified": "2025-02-08T18:27:32-05:00",
    "input_file": "module-4-assignment.knit.md"
  },
  {
    "path": "posts/2025-02-01-module-3-assignment/",
    "title": "Module-3-Assignment",
    "description": "Descriptive statistics: central tendency & variation.",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-02-01",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 2/1/2025\r\n\r\n\r\n\r\n# Define two sets of data\r\nfirstSet <- c(10, 2, 3, 2, 4, 2, 5)\r\nfirstSet\r\n\r\n[1] 10  2  3  2  4  2  5\r\n\r\nsecondSet <- c(20, 12, 13, 12, 14, 12, 15)\r\nsecondSet\r\n\r\n[1] 20 12 13 12 14 12 15\r\n\r\n# A1.\r\n# \"Central Tendency\"\r\n# Solve the mean, median, and mode of both data sets\r\n# First set mean:\r\nmean(firstSet)\r\n\r\n[1] 4\r\n\r\n# First set median:\r\nmedian(firstSet)\r\n\r\n[1] 3\r\n\r\n# First set mode:\r\nMode(firstSet)\r\n\r\n[1] 2\r\nattr(,\"freq\")\r\n[1] 3\r\n\r\n# Second set mean:\r\nmean(secondSet)\r\n\r\n[1] 14\r\n\r\n# Second set median:\r\nmedian(secondSet)\r\n\r\n[1] 13\r\n\r\n# Second set mode:\r\nMode(secondSet)\r\n\r\n[1] 12\r\nattr(,\"freq\")\r\n[1] 3\r\n\r\n# A2.\r\n# \"Variation\"\r\n# Compute the range, interquartile, variance,\r\n# and standard deviation for both sets.\r\n# First set min and max:\r\nfirstSetMinMax <- range(firstSet)\r\nfirstSetMinMax\r\n\r\n[1]  2 10\r\n\r\n# First set range:\r\nfirstSetRange <- max(firstSet) - min(firstSet)\r\nfirstSetRange\r\n\r\n[1] 8\r\n\r\n# First set quartiles:\r\nquantile(firstSet)\r\n\r\n  0%  25%  50%  75% 100% \r\n 2.0  2.0  3.0  4.5 10.0 \r\n\r\n# First set interquartile range:\r\nIQR(firstSet)\r\n\r\n[1] 2.5\r\n\r\n# First set variance:\r\nvar(firstSet)\r\n\r\n[1] 8.333333\r\n\r\n# First set standard deviation:\r\nsd(firstSet)\r\n\r\n[1] 2.886751\r\n\r\n# Second set min and max:\r\nsecondSetMinMax <- range(secondSet)\r\nsecondSetMinMax\r\n\r\n[1] 12 20\r\n\r\n# Second set range:\r\nsecondSetRange <- max(secondSet) - min(secondSet)\r\nsecondSetRange\r\n\r\n[1] 8\r\n\r\n# Second set quartiles:\r\nquantile(secondSet)\r\n\r\n  0%  25%  50%  75% 100% \r\n12.0 12.0 13.0 14.5 20.0 \r\n\r\n# Second set interquartile range:\r\nIQR(secondSet)\r\n\r\n[1] 2.5\r\n\r\n# Second set variance:\r\nvar(secondSet)\r\n\r\n[1] 8.333333\r\n\r\n# Second set standard deviation:\r\nsd(secondSet)\r\n\r\n[1] 2.886751\r\n\r\n# A3.\r\n# \"Comparison\"\r\n# Compare results between the first and second set by\r\n# discussing the differences between the two sets.\r\nsummary(firstSet)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n    2.0     2.0     3.0     4.0     4.5    10.0 \r\n\r\nsummary(secondSet)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   12.0    12.0    13.0    14.0    14.5    20.0 \r\n\r\n# Although both sets have the same range, standard deviation,\r\n# variance, and interquartile range, they have a different\r\n# mean, which will affect the coefficient of variance.\r\n# Coefficient of variation as percent:\r\n# First set:\r\nsd(firstSet)/mean(firstSet)*100\r\n\r\n[1] 72.16878\r\n\r\n# Second set:\r\nsd(secondSet)/mean(secondSet)*100\r\n\r\n[1] 20.61965\r\n\r\n# Because the first set has a larger coefficient of variation...\r\n# the data is spread away from the mean on a larger interval.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/Rplot-1.jpeg?raw=true",
    "last_modified": "2025-02-01T21:57:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2025-01-24-module-2-assignment/",
    "title": "Module 2 Assignment",
    "description": "The myMean function calculates the mean.",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-01-24",
    "categories": [],
    "contents": "\r\n\r\n\r\n# Name: Skylar Walsh\r\n# Professor: Lingyao Li\r\n# Course: LIS4273.002\r\n# Date: 1/24/2025\r\n\r\n# The variable assignment2 has a vector with a total of 9 elements\r\nassignment2 <- c(6, 18, 14, 22, 27, 17, 22, 20, 22)\r\n\r\n# Define a function called myMean that will calculate the average\r\n# of a vector. In the body of the function, we are adding all of\r\n# the elements from the assignment2 variable and then dividing by how many\r\n# items are in assignment2\r\nmyMean <- function(assignment2) {return(sum(assignment2)/length(assignment2))}\r\n\r\n# Run the function, print the mean result into the console.\r\nmyMean(assignment2)\r\n\r\n[1] 18.66667\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/mean-r.png?raw=true",
    "last_modified": "2025-02-01T20:56:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "My LIS4273 Blog",
    "description": "This is for LIS4273 advanced stats.",
    "author": [
      {
        "name": "Skylar Walsh",
        "url": {}
      }
    ],
    "date": "2025-01-24",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/skywalshUSF/LIS4273_sWalsh_Blog/blob/main/IMG1.jpg?raw=true",
    "last_modified": "2025-02-01T18:50:34-05:00",
    "input_file": {}
  }
]
